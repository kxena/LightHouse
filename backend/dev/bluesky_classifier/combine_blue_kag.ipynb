{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdaa2fb2",
   "metadata": {},
   "source": [
    "# Trained on combined 200 kaggle & bluesky training data, tested on ~340 bluesky data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d446d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d57bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (200, 4)\n",
      "Testing data shape: (341, 3)\n",
      "\n",
      "First few rows of training data:\n",
      "   id     keyword                                               text  target\n",
      "0   1  earthquake  Magnitude 6.9 shook the central Philippines la...       1\n",
      "1   2  earthquake  6.7 Magnitude Earthquake hit our region hard.....       1\n",
      "2   3       flood  Hey, @democrats.org - the constant flood of pl...       0\n",
      "3   4  earthquake  New earthquake reported: M 6.0 - 32 km SE of K...       1\n",
      "4   5        none  It‚Äôs up to people to #RiseForGaza\\nSHUT IT DOW...       0\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing data\n",
    "train_df = pd.read_csv('kag_blue_combined_train.csv')\n",
    "test_df = pd.read_csv('test1.csv')\n",
    "\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Testing data shape:\", test_df.shape)\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95cc349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of cleaned training text:\n",
      "Original: Magnitude 6.9 shook the central Philippines late Tuesday, sending panicked people dashing out into streets, damaging a stone church and prompting a local tsunami warning. The earthquake was centered about 17 kilometers northeast of Bogo city in Cebu.\n",
      "Cleaned: magnitude shook the central philippines late tuesday sending panicked people dashing out into streets damaging a stone church and prompting a local tsunami warning the earthquake was centered about kilometers northeast of bogo city in cebu\n",
      "\n",
      "Example of cleaned test text:\n",
      "Original: New earthquake reported: M 4.9 - 79 km NNE of Popondetta, Papua New Guinea - 2025-10-06T17:36:45.040Z\n",
      "Cleaned: new earthquake reported m km nne of popondetta papua new guinea 06t17040z\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses tweet text by:\n",
    "    - Converting to lowercase\n",
    "    - Removing URLs\n",
    "    - Removing mentions (@username)\n",
    "    - Removing emojis\n",
    "    - Removing standalone numbers (magnitudes, depths, coordinates)\n",
    "    - Removing dates and times\n",
    "    - Removing special characters and punctuation\n",
    "    - Removing extra whitespace\n",
    "    - Keeping meaningful labels (location, magnitude, depth, etc.)\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags (keep the word, remove the #)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remove emojis (comprehensive pattern for all emoji ranges)\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"\\U00002702-\\U000027B0\"  # dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # supplemental symbols\n",
    "        \"\\U0001FA00-\\U0001FAFF\"  # extended symbols\n",
    "        \"\\U00002500-\\U00002BEF\"  # additional symbols\n",
    "        \"\\U0001F000-\\U0001F02F\"  # mahjong tiles\n",
    "        \"\\U0001F0A0-\\U0001F0FF\"  # playing cards\n",
    "        \"\\U00002300-\\U000023FF\"  # miscellaneous technical (like ‚è∞)\n",
    "        \"\\U00002B00-\\U00002BFF\"  # miscellaneous symbols and arrows\n",
    "        \"\\U0000FE00-\\U0000FE0F\"  # variation selectors\n",
    "        \"\\U0001F200-\\U0001F2FF\"  # enclosed ideographic supplement\n",
    "        \"]+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Remove standalone numbers that might be coordinates, magnitudes, depths\n",
    "    # But keep words with numbers (like \"2day\" or \"gr8\")\n",
    "    text = re.sub(r'\\b\\d+\\.?\\d*\\b', '', text)\n",
    "    \n",
    "    # Remove dates and times (various formats)\n",
    "    text = re.sub(r'\\d{4}-\\d{2}-\\d{2}', '', text)\n",
    "    text = re.sub(r'\\d{2}:\\d{2}:\\d{2}', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to training and testing data\n",
    "train_df['cleaned_text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\nExample of cleaned training text:\")\n",
    "print(f\"Original: {train_df['text'].iloc[0]}\")\n",
    "print(f\"Cleaned: {train_df['cleaned_text'].iloc[0]}\")\n",
    "\n",
    "print(\"\\nExample of cleaned test text:\")\n",
    "print(f\"Original: {test_df['text'].iloc[0]}\")\n",
    "print(f\"Cleaned: {test_df['cleaned_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5707f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF feature matrix shape (training): (200, 479)\n",
      "TF-IDF feature matrix shape (testing): (341, 479)\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction using TF-IDF\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit to top 5000 features\n",
    "    min_df=2,           # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.8,         # Ignore terms that appear in more than 80% of documents\n",
    "    ngram_range=(1, 2), # Use unigrams and bigrams\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train = tfidf.fit_transform(train_df['cleaned_text'])\n",
    "y_train = train_df['target']\n",
    "\n",
    "# Transform testing data (use same vectorizer)\n",
    "X_test = tfidf.transform(test_df['cleaned_text'])\n",
    "\n",
    "print(f\"\\nTF-IDF feature matrix shape (training): {X_train.shape}\")\n",
    "print(f\"TF-IDF feature matrix shape (testing): {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621da108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 160\n",
      "Validation set size: 40\n"
     ]
    }
   ],
   "source": [
    "# Split training data to create a validation set\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train_split.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edc74902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model...\n",
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    C=1.0,  # Regularization parameter\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "print(\"\\nTraining the model...\")\n",
    "model.fit(X_train_split, y_train_split)\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e10f464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.7500\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Disaster       0.75      0.75      0.75        20\n",
      "    Disaster       0.75      0.75      0.75        20\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.75      0.75      0.75        40\n",
      "weighted avg       0.75      0.75      0.75        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15  5]\n",
      " [ 5 15]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on Validation set\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"\\nValidation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=['Not Disaster', 'Disaster']))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd3d829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model on full training data...\n",
      "Final model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train model on full training data\n",
    "print(\"\\nTraining final model on full training data...\")\n",
    "final_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    C=1.0\n",
    ")\n",
    "final_model.fit(X_train, y_train)\n",
    "print(\"Final model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "421b755e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of predictions: 341\n",
      "Predicted disasters: 213\n",
      "Predicted non-disasters: 128\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "print(f\"\\nNumber of predictions: {len(test_predictions)}\")\n",
    "print(f\"Predicted disasters: {sum(test_predictions)}\")\n",
    "print(f\"Predicted non-disasters: {len(test_predictions) - sum(test_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f165ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results file created: results1.csv\n",
      "           id                                               text  target\n",
      "0  028e52b32c  New earthquake reported: M 4.9 - 79 km NNE of ...       1\n",
      "1  2fff02eced  New earthquake reported: M 4.3 - 56 km SE of M...       1\n",
      "2  80109009d2  Earthquake Location: OFF EAST COAST OF KAMCHAT...       1\n",
      "3  f7a8e7d8d5  Earthquake Location: OFF EAST COAST OF KAMCHAT...       1\n",
      "4  763fca34c1  Reviewed M 1.1 earthquake in Grady County, Okl...       1\n",
      "5  2490bfc09d  #Earthquake M5.1 | Russia: Off East Coast of K...       1\n",
      "6  237bb90a61  Earthquake Report\\n\\nEarthquake M5.1 has been ...       1\n",
      "7  06154f4926  Reviewed M 1.8 earthquake in Alfalfa County, O...       1\n",
      "8  eeb64ad17c  üåç Earthquake Alert üåç\\nüìç Location: OFF EAST COA...       1\n",
      "9  05198e8418  Magnitude : 5.3\\nRegion: *Off East Coast of Ka...       1\n"
     ]
    }
   ],
   "source": [
    "# Create CSV for predicted results\n",
    "results = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'text': test_df['text'],\n",
    "    'target': test_predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results.to_csv('results1.csv', index=False)\n",
    "print(\"\\nResults file created: results1.csv\")\n",
    "print(results.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
